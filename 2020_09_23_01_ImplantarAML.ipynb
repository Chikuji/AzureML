{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2020-09-23-01-ImplantarAML.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO3ax2Ps3D6jmSqpfhG0x9k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chikuji/AzureML/blob/master/2020_09_23_01_ImplantarAML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBrjShDrvp1x",
        "colab_type": "text"
      },
      "source": [
        "# Objetivos de aprendizagem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyOn541bvsgO",
        "colab_type": "text"
      },
      "source": [
        "Neste módulo, você vai aprender a:\n",
        "\n",
        "- Implantar um modelo como um serviço de inferência em tempo real.\n",
        "-Consumir um serviço de inferência em tempo real.\n",
        "-Solucionar problemas de implantação de serviço"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRnCZo43ypsP",
        "colab_type": "text"
      },
      "source": [
        "Como implantar um modelo como um serviço em tempo real"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t8XFEGD0CIQ",
        "colab_type": "text"
      },
      "source": [
        "# 1. Registrar um modelo treinado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lM08mjXW0J4q",
        "colab_type": "text"
      },
      "source": [
        "Depois de treinar um modelo com êxito, você precisará registrá-lo em seu Workspace do Azure Machine Learning. O serviço em tempo real poderá, então, carregar o modelo quando necessário."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F02Qkiy8vr_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from azureml.core import Model\n",
        "\n",
        "classification_model = Model.register(workspace=ws,\n",
        "                       model_name='classification_model',\n",
        "                       model_path='model.pkl', # local path\n",
        "                       description='A classification model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGiJUYC-0MnK",
        "colab_type": "text"
      },
      "source": [
        "# 2. Definir uma configuração de inferência"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5T87_ONa0Rgo",
        "colab_type": "text"
      },
      "source": [
        "O modelo será implantado como um serviço que consiste em:\n",
        "\n",
        "- Um script para carregar o modelo e retornar previsões para os dados enviados\n",
        "- Um ambiente no qual o script será executado.\n",
        "\n",
        "Portanto, você precisará definir o script e o ambiente para o serviço."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COngf0dt5XLQ",
        "colab_type": "text"
      },
      "source": [
        "Como criar um script de entrada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdMXNLKmvYUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import joblib\n",
        "import numpy as np\n",
        "from azureml.core.model import Model\n",
        "\n",
        "# Called when the service is loaded\n",
        "def init():\n",
        "    global model\n",
        "    # Get the path to the registered model file and load it\n",
        "    model_path = Model.get_model_path('classification_model')\n",
        "    model = joblib.load(model_path)\n",
        "\n",
        "# Called when a request is received\n",
        "def run(raw_data):\n",
        "    # Get the input data as a numpy array\n",
        "    data = np.array(json.loads(raw_data)['data'])\n",
        "    # Get a prediction from the model\n",
        "    predictions = model.predict(data)\n",
        "    # Return the predictions as any JSON serializable format\n",
        "    return predictions.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwzCxyYu5ouP",
        "colab_type": "text"
      },
      "source": [
        "Como criar um ambiente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Fh_eYr_6IYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "\n",
        "# Add the dependencies for your model\n",
        "myenv = CondaDependencies()\n",
        "myenv.add_conda_package(\"scikit-learn\")\n",
        "\n",
        "# Save the environment config as a .yml file\n",
        "env_file = 'service_files/env.yml'\n",
        "with open(env_file,\"w\") as f:\n",
        "    f.write(myenv.serialize_to_string())\n",
        "print(\"Saved dependency info in\", env_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0s8eMzrV6NcQ",
        "colab_type": "text"
      },
      "source": [
        "Como combinar o script e o ambiente em uma InferenceConfig"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1t93L87h6ao3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from azureml.core.model import InferenceConfig\n",
        "\n",
        "classifier_inference_config = InferenceConfig(runtime= \"python\",\n",
        "                                              source_directory = 'service_files',\n",
        "                                              entry_script=\"score.py\",\n",
        "                                              conda_file=\"env.yml\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH7ziKZ98ju8",
        "colab_type": "text"
      },
      "source": [
        "# 3. Definir uma configuração de implantação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFvnR-Ul8njQ",
        "colab_type": "text"
      },
      "source": [
        "Agora que você tem o script de entrada e o ambiente, é necessário configurar a computação na qual o serviço será implantado. Se você estiver fazendo a implantação em um cluster do AKS, precisará criar o cluster e um destino de computação para ele antes da implantação:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaIoYUTX8oov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from azureml.core.compute import ComputeTarget, AksCompute\n",
        "\n",
        "cluster_name = 'aks-cluster'\n",
        "compute_config = AksCompute.provisioning_configuration(location='eastus')\n",
        "production_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
        "production_cluster.wait_for_completion(show_output=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v522FQDR8zoq",
        "colab_type": "text"
      },
      "source": [
        "Com o destino de computação criado, agora você poderá definir a configuração de implantação, que define a especificação de computação específica do destino para a implantação em contêineres:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO0XmGku8041",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from azureml.core.webservice import AksWebservice\n",
        "\n",
        "classifier_deploy_config = AksWebservice.deploy_configuration(cpu_cores = 1,\n",
        "                                                              memory_gb = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MZELCOl_ZP9",
        "colab_type": "text"
      },
      "source": [
        "# 4. Implantar o modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWyQg2bM_bHU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from azureml.core.model import Model\n",
        "\n",
        "model = ws.models['classification_model']\n",
        "service = Model.deploy(workspace=ws,\n",
        "                       name = 'classifier-service',\n",
        "                       models = [model],\n",
        "                       inference_config = classifier_inference_config,\n",
        "                       deployment_config = classifier_deploy_config,\n",
        "                       deployment_target = production_cluster)\n",
        "service.wait_for_deployment(show_output = True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uxj0Gd8ZBMwP",
        "colab_type": "text"
      },
      "source": [
        "Para os serviços locais ou da ACI, você poderá omitir o parâmetro deployment_target (ou defini-lo como Nenhum).\n",
        "\n",
        "Mais informações: Para obter mais informações sobre como implantar modelos com o Azure Machine Learning, confira Implantar modelos com o Azure Machine Learning na documentação."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9-ALcb8Bq-I",
        "colab_type": "text"
      },
      "source": [
        "# Como usar o SDK do Azure Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeOJeW1fDDAB",
        "colab_type": "text"
      },
      "source": [
        "Para teste, use o SDK do Azure Machine Learning para chamar um serviço Web por meio do método run de um objeto WebService que referencia o serviço implantado. Normalmente, você envia dados para o método run no formato JSON com a seguinte estrutura:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UvJpaNCDERf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "{\n",
        "  \"data\":[\n",
        "      [0.1,2.3,4.1,2.0], // 1st case\n",
        "      [0.2,1.8,3.9,2.1],  // 2nd case,\n",
        "      ...\n",
        "  ]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2XLB5FdDJ_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "# An array of new data cases\n",
        "x_new = [[0.1,2.3,4.1,2.0],\n",
        "         [0.2,1.8,3.9,2.1]]\n",
        "\n",
        "# Convert the array to a serializable list in a JSON document\n",
        "json_data = json.dumps({\"data\": x_new})\n",
        "\n",
        "# Call the web service, passing the input data\n",
        "response = service.run(input_data = json_data)\n",
        "\n",
        "# Get the predictions\n",
        "predictions = json.loads(response)\n",
        "\n",
        "# Print the predicted class for each case.\n",
        "for i in range(len(x_new)):\n",
        "    print (x_new[i]), predictions[i] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igyb4AktDhi5",
        "colab_type": "text"
      },
      "source": [
        "# Como usar um ponto de extremidade REST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7x1q3k6D2s0",
        "colab_type": "text"
      },
      "source": [
        "Em produção, a maioria dos aplicativos cliente não incluirá o SDK do Azure Machine Learning e consumirá o serviço por meio da interface REST. Você pode determinar o ponto de extremidade de um serviço implantado no Azure Machine Learning Studio ou recuperando a propriedade scoring_uri do objeto Webservice no SDK da seguinte maneira:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hxRg2wHDjF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "endpoint = service.scoring_uri\n",
        "print(endpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV9t3XwnD6Q2",
        "colab_type": "text"
      },
      "source": [
        "Com o ponto de extremidade conhecido, você pode usar uma solicitação HTTP POST com os dados JSON para chamar o serviço. O seguinte exemplo mostra como fazer isso usando o Python:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9XFLMCuD723",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# An array of new data cases\n",
        "x_new = [[0.1,2.3,4.1,2.0],\n",
        "         [0.2,1.8,3.9,2.1]]\n",
        "\n",
        "# Convert the array to a serializable list in a JSON document\n",
        "json_data = json.dumps({\"data\": x_new})\n",
        "\n",
        "# Set the content type in the request headers\n",
        "request_headers = { 'Content-Type':'application/json' }\n",
        "\n",
        "# Call the service\n",
        "response = requests.post(url = endpoint,\n",
        "                         data = json_data,\n",
        "                         headers = request_headers)\n",
        "\n",
        "# Get the predictions from the JSON response\n",
        "predictions = json.loads(response.json())\n",
        "\n",
        "# Print the predicted class for each case.\n",
        "for i in range(len(x_new)):\n",
        "    print (x_new[i]), predictions[i] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91ouKacJEZXn",
        "colab_type": "text"
      },
      "source": [
        "# Autenticação\n",
        "\n",
        "- Chave: as solicitações são autenticadas especificando a chave associada ao serviço.\n",
        "-Token: as solicitações são autenticadas pelo fornecimento de um JWT (Token Web JSON)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxW_Hup5EfuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "primary_key, secondary_key = service.get_keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G02aYUSeEg0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# An array of new data cases\n",
        "x_new = [[0.1,2.3,4.1,2.0],\n",
        "         [0.2,1.8,3.9,2.1]]\n",
        "\n",
        "# Convert the array to a serializable list in a JSON document\n",
        "json_data = json.dumps({\"data\": x_new})\n",
        "\n",
        "# Set the content type in the request headers\n",
        "request_headers = { \"Content-Type\":\"application/json\",\n",
        "                    \"Authorization\":\"Bearer \" + key_or_token }\n",
        "\n",
        "# Call the service\n",
        "response = requests.post(url = endpoint,\n",
        "                         data = json_data,\n",
        "                         headers = request_headers)\n",
        "\n",
        "# Get the predictions from the JSON response\n",
        "predictions = json.loads(response.json())\n",
        "\n",
        "# Print the predicted class for each case.\n",
        "for i in range(len(x_new)):\n",
        "    print (x_new[i]), predictions[i] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q4r8VxhE_SN",
        "colab_type": "text"
      },
      "source": [
        "# Solução de problemas de implantação de serviço"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXkUvOJJFAZz",
        "colab_type": "text"
      },
      "source": [
        "Verificar o estado do serviço"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYaj1KcMFAxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from azureml.core.webservice import AksWebservice\n",
        "\n",
        "# Get the deployed service\n",
        "service = AciWebservice(name='classifier-service', workspace=ws)\n",
        "\n",
        "# Check its state\n",
        "print(service.state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcbEDmu7FC76",
        "colab_type": "text"
      },
      "source": [
        "Examinar os logs de serviço"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGMVDrhDFDuR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(service.get_logs())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_r1IpkOFGZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from azureml.core.webservice import LocalWebservice\n",
        "\n",
        "deployment_config = LocalWebservice.deploy_configuration(port=8890)\n",
        "service = Model.deploy(ws, 'test-svc', [model], inference_config, deployment_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlnI4Tj1FJRw",
        "colab_type": "text"
      },
      "source": [
        "Em seguida, você poderá testar o serviço implantado localmente usando o SDK:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Chd2iLDwFKPR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(service.run(input_data = json_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lczM6Tq0FKu2",
        "colab_type": "text"
      },
      "source": [
        "Em seguida, você poderá solucionar problemas de runtime fazendo alterações no arquivo de pontuação referenciado na configuração de inferência e recarregando o serviço sem reimplantá-lo (algo que só pode ser feito com um serviço local):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0sJ3CrWFMbd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "service.reload()\n",
        "print(service.run(input_data = json_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdhuOfdlFF2C",
        "colab_type": "text"
      },
      "source": [
        "Implantação em um contêiner local"
      ]
    }
  ]
}